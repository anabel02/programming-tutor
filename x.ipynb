{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vertexai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreview\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rag\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreview\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerative_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerativeModel, Tool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vertexai'"
     ]
    }
   ],
   "source": [
    "from vertexai.preview import rag\n",
    "from vertexai.preview.generative_models import GenerativeModel, Tool\n",
    "import vertexai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize Vertex AI API\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your project ID\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "# Create a RAG Corpus\n",
    "display_name = \"test_corpus\"\n",
    "paths = [\"gs://my_bucket/my_files_dir\"]  # Path to your files in Google Cloud Storage\n",
    "\n",
    "# Configure embedding model\n",
    "embedding_model_config = rag.EmbeddingModelConfig(\n",
    "    publisher_model=\"publishers/google/models/text-embedding-004\"\n",
    ")\n",
    "\n",
    "rag_corpus = rag.create_corpus(\n",
    "    display_name=display_name,\n",
    "    embedding_model_config=embedding_model_config,\n",
    ")\n",
    "\n",
    "# Import files into the RagCorpus\n",
    "rag.import_files(\n",
    "    rag_corpus.name,\n",
    "    paths,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    max_embedding_requests_per_min=900,\n",
    ")\n",
    "\n",
    "# Create a retrieval query\n",
    "response = rag.retrieval_query(\n",
    "    rag_resources=[rag.RagResource(rag_corpus=rag_corpus.name)],\n",
    "    text=\"What is RAG and why it is helpful?\",\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# Print the retrieval response\n",
    "print(\"Retrieval Response:\", response)\n",
    "\n",
    "# Create a RAG retrieval tool\n",
    "rag_retrieval_tool = Tool.from_retrieval(\n",
    "    retrieval=rag.Retrieval(source=rag.VertexRagStore(\n",
    "        rag_resources=[rag.RagResource(rag_corpus=rag_corpus.name)],\n",
    "        similarity_top_k=3,\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Create a Gemini model instance for generation\n",
    "rag_model = GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash-001\",\n",
    "    tools=[rag_retrieval_tool]\n",
    ")\n",
    "\n",
    "# Generate a response based on retrieved context\n",
    "generation_response = rag_model.generate_content(\"What is RAG and why it is helpful?\")\n",
    "print(\"Generated Response:\", generation_response.text)\n",
    "\n",
    "# Setting up LangChain for enhanced question answering\n",
    "system_prompt = (\n",
    "    \"You are an expert. Provide clear, concise answers based on the provided context. \"\n",
    "    \"If the information is not found in the context, state that the answer is unavailable.\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    ")\n",
    "\n",
    "# Create a question-answering chain with LangChain\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
    "chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "rag_chain = create_retrieval_chain(retriever=None, chain=chain)  # Replace `None` with your retriever\n",
    "\n",
    "# Ask a question using the RAG chain\n",
    "final_response = rag_chain.invoke({\"input\": \"What are mutual funds?\"})\n",
    "print(\"Final Response:\", final_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
