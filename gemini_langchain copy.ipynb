{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community\n",
    "\n",
    "!pip install langchain_google_genai\n",
    "\n",
    "!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "PDF_PATH = \"Cap 07 Arrays.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(PDF_PATH)  # Load your PDF file\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"Total number of Chunks: \", len(docs))  # Check how many chunks we have\n",
    "for chunk in docs:\n",
    "    print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = \"AIzaSyCoxFsjIYKIz0jxIwlHYR5tI1by7LRvqw4\"\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set. Please set it as an environment variable.\")\n",
    "\n",
    "# Load the Gemini API key\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
    "\n",
    "# Test embedding a query\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "print(len(vector))\n",
    "print(vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "# PERSISTENT_DIRECTORY = \"chroma\"\n",
    "\n",
    "# vectorstoredb = Chroma.from_documents(\n",
    "#     documents=docs, embedding=embeddings, persist_directory=PERSISTENT_DIRECTORY\n",
    "# )\n",
    "\n",
    "# retriever = vectorstoredb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "PERSISTENT_DIRECTORY = \"chroma\"\n",
    "\n",
    "# Check if the persistent directory already exists (indicating the vector store is already created)\n",
    "if not os.path.exists(PERSISTENT_DIRECTORY):\n",
    "    # If the vector store does not exist, you need to create it\n",
    "    # Example: you need to load your documents and embeddings\n",
    "    # docs = ...  # Load your documents (e.g., list of text documents)\n",
    "    # embeddings = OpenAIEmbeddings()  # Use your embeddings function\n",
    "\n",
    "    # For the sake of example, let's assume you have a list of documents called `docs`\n",
    "\n",
    "    # Create the vector store by embedding the documents and persisting them\n",
    "    vectorstoredb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=PERSISTENT_DIRECTORY)\n",
    "else:\n",
    "    # If the vector store already exists, load it\n",
    "    vectorstoredb = Chroma(persist_directory=PERSISTENT_DIRECTORY, embedding_function=embeddings)\n",
    "\n",
    "# Set up the retriever for similarity search (retrieving the top 5 most similar documents)\n",
    "retriever = vectorstoredb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"array bidimensional\")\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content)  # Print the first retrieved document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a system prompt\n",
    "system_prompt = (\n",
    "   ''''\n",
    "You are a knowledgeable AI tutor, dedicated to answering questions in a clear and thorough manner.\n",
    "Your goal is to break down complex concepts into simple, easy-to-understand terms, making them suitable for a non-technical audience.\n",
    "Maintain a warm, conversational tone, guiding the student step by step.\n",
    "\n",
    "Your responses must be based exclusively on the content from the passage and the examples included in it.\n",
    "If the passage does not address the question, kindly explain that the answer is not available in the provided material.\n",
    "\n",
    "Respond in Spanish, ensuring that the explanation is simple and easy to follow.\n",
    "The topic is C# programming, so focus on simplifying and clarifying relevant concepts.\n",
    "\n",
    "At the end of your answer, include a reference to the source (document name) and the pages that the passage was taken from.\n",
    "Do not add any additional information.\n",
    "\n",
    "---\n",
    "    {context}'''\n",
    ")\n",
    "\n",
    "# Set up the prompt for the QA chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"que es un array bidimensional\"})\n",
    "# print(response)\n",
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
