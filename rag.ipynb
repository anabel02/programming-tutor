{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community\n",
    "\n",
    "!pip install langchain_google_genai\n",
    "\n",
    "!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "FOLDER_PATH = \"content\"\n",
    "\n",
    "def load_pdf(path):\n",
    "    loader = PyPDFLoader(path)  # Load your PDF file\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "def load_all_pdfs_in_folder(folder_path):\n",
    "    # List all PDF files in the folder\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    \n",
    "    # Initialize a list to store the loaded documents\n",
    "    all_documents = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        print(f\"Loading PDF: {pdf_path}\")\n",
    "        documents = load_pdf(pdf_path)  # Load the PDF\n",
    "        all_documents.extend(documents)  # Add the loaded documents to the list\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "\n",
    "def split_text(data):\n",
    "    docs = text_splitter.split_documents(data)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = \"AIzaSyCoxFsjIYKIz0jxIwlHYR5tI1by7LRvqw4\"\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set. Please set it as an environment variable.\")\n",
    "\n",
    "# Load the Gemini API key\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "PERSISTENT_DIRECTORY = \"chroma\"\n",
    "\n",
    "# Check if the persistent directory already exists (indicating the vector store is already created)\n",
    "if not os.path.exists(PERSISTENT_DIRECTORY):\n",
    "    # If the vector store does not exist, you need to create it\n",
    "    # Example: you need to load your documents and embeddings\n",
    "    # docs = ...  # Load your documents (e.g., list of text documents)\n",
    "    # embeddings = OpenAIEmbeddings()  # Use your embeddings function\n",
    "\n",
    "    # For the sake of example, let's assume you have a list of documents called `docs`\n",
    "\n",
    "    # Create the vector store by embedding the documents and persisting them\n",
    "    data = load_all_pdfs_in_folder(FOLDER_PATH)\n",
    "    docs = split_text(data)\n",
    "    print(docs)\n",
    "    vectorstoredb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=PERSISTENT_DIRECTORY)\n",
    "else:\n",
    "    # If the vector store already exists, load it\n",
    "    vectorstoredb = Chroma(persist_directory=PERSISTENT_DIRECTORY, embedding_function=embeddings)\n",
    "\n",
    "# Set up the retriever for similarity search (retrieving the top 5 most similar documents)\n",
    "retriever = vectorstoredb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"array bidimensional\")\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content)  # Print the first retrieved document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a system prompt\n",
    "system_prompt = (\n",
    "   ''''\n",
    "You are a knowledgeable AI tutor, dedicated to answering questions in a clear and thorough manner.\n",
    "Your goal is to break down complex concepts into simple, easy-to-understand terms, making them suitable for a non-technical audience.\n",
    "Maintain a warm, conversational tone, guiding the student step by step.\n",
    "\n",
    "Your responses must be based exclusively on the content from the passage and the examples included in it.\n",
    "If the passage does not address the question, kindly explain that the answer is not available in the provided material.\n",
    "\n",
    "Respond in Spanish, ensuring that the explanation is simple and easy to follow.\n",
    "The topic is C# programming, so focus on simplifying and clarifying relevant concepts.\n",
    "\n",
    "At the end of your answer, include a reference to the source (document name) and the pages that the passage was taken from.\n",
    "Do not add any additional information.\n",
    "\n",
    "---\n",
    "    {context}'''\n",
    ")\n",
    "\n",
    "# Set up the prompt for the QA chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"qu√© es hola mundo\"})\n",
    "# print(response)\n",
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
